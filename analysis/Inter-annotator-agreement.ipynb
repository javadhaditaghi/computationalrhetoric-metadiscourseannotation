{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "Inter-Annotator Agreement Analysis\n",
    "Comparing Human vs LLM Role Annotations\n",
    "This notebook calculates inter-annotator agreement between human and LLM annotations for the 'role' column.\n"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# Import required libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.metrics import cohen_kappa_score, accuracy_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "from scipy.stats import chi2_contingency\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "# Set up plotting style\n",
    "plt.style.use('default')\n",
    "sns.set_palette(\"husl\")\n",
    "plt.rcParams['figure.figsize'] = (10, 6)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Annotations:\n",
      "Shape: (25, 9)\n",
      "Columns: ['sentence', 'context_before', 'context_after', 'section', 'thesis_code', 'expression', 'role', 'justification', 'Note ']\n",
      "\n",
      "First few rows:\n",
      "                                            sentence  \\\n",
      "0  This thesis focuses on ‘text as an instrument’...   \n",
      "1  This study takes the standpoint that genre is ...   \n",
      "2  Scacci (2011) states, ‘genres exist in relatio...   \n",
      "3  The present study aims to fill this gap within...   \n",
      "4                 Below, Figure 1.1 visualises this:   \n",
      "\n",
      "                                      context_before  \\\n",
      "0  “To a grammarian, text is a rich, many-faceted...   \n",
      "1  “To a grammarian, text is a rich, many-faceted...   \n",
      "2  And this study sees this corpus, the American ...   \n",
      "3  Since the address is delivered on Saturdays, t...   \n",
      "4  Moreover, Scacco (2011) analysed Obama’s radio...   \n",
      "\n",
      "                                       context_after       section  \\\n",
      "0  The study postulates that sub-genres exist wit...  Introduction   \n",
      "1  It is further elaborated as “a staged, goal-or...  Introduction   \n",
      "2  The radio address thus is realised, operates a...  Introduction   \n",
      "3  The present study focuses on the Radio Address...  Introduction   \n",
      "4  Genre: American Presidential Radio Address Sub...  Introduction   \n",
      "\n",
      "  thesis_code                          expression           role  \\\n",
      "0         AP1                This thesis focuses…  Metadiscourse   \n",
      "1         AP1  (this study) takes the standpoint…  Metadiscourse   \n",
      "2         AP1               Scacci (2011) states…     Borderline   \n",
      "3         AP1              aims to fill this gap…  Metadiscourse   \n",
      "4         AP1                              Below…  Metadiscourse   \n",
      "\n",
      "                                       justification Note   \n",
      "0           Organize the study's focus and objective   NaN  \n",
      "1  establishing the study's theoretical framework...   NaN  \n",
      "2  Scacci (2011) states' serves both as a reporti...   NaN  \n",
      "3  The expression 'aims to fill this gap' is a cl...   NaN  \n",
      "4  The expression does not convey factual content...   NaN  \n",
      "\n",
      "==================================================\n",
      "LLM Annotations:\n",
      "Shape: (25, 15)\n",
      "Columns: ['thesis_code', 'section', 'sentence', 'expression', 'claude_metadiscourse_annotation', 'gemini_metadiscourse_annotation', 'deepseek_metadiscourse_annotation', 'context_before', 'context_after', 'optimized_final_decision', 'role', 'confidence', 'note', 'justification', 'context_assessment']\n",
      "\n",
      "First few rows:\n",
      "  thesis_code       section  \\\n",
      "0         AP1  Introduction   \n",
      "1         AP1  Introduction   \n",
      "2         AP1  Introduction   \n",
      "3         AP1  Introduction   \n",
      "4         AP1  Introduction   \n",
      "\n",
      "                                            sentence          expression  \\\n",
      "0                 Below, Figure 1.1 visualises this:              Below…   \n",
      "1  Finally, the structure of the overall thesis w...            Finally…   \n",
      "2  Further, Hasan (2001: 8) maintains that contex...        First… then…   \n",
      "3  Halliday (2009:56) explains that language prov...  Halliday…explains…   \n",
      "4  Hasan (2001: 8) maintains that context (of sit...     maintains that…   \n",
      "\n",
      "                     claude_metadiscourse_annotation  \\\n",
      "0  ```json\\n{\\n   \"role\": \"Metadiscourse\",\\n   \"c...   \n",
      "1  ```json\\n{\\n   \"role\": \"Metadiscourse\",\\n   \"c...   \n",
      "2  ```json\\n{\\n   \"role\": \"Propositional\",\\n   \"c...   \n",
      "3  ```json\\n{\\n   \"role\": \"Propositional\",\\n   \"c...   \n",
      "4  ```json\\n{\\n   \"role\": \"Propositional\",\\n   \"c...   \n",
      "\n",
      "                     gemini_metadiscourse_annotation  \\\n",
      "0  ```json\\n{\\n  \"role\": \"Metadiscourse\",\\n  \"con...   \n",
      "1  ```json\\n{\\n  \"role\": \"Metadiscourse\",\\n  \"con...   \n",
      "2  ```json\\n{\\n  \"role\": \"Metadiscourse\",\\n  \"con...   \n",
      "3  ```json\\n{\\n  \"role\": \"Metadiscourse\",\\n  \"con...   \n",
      "4  ```json\\n{\\n  \"role\": \"Metadiscourse\",\\n  \"con...   \n",
      "\n",
      "                   deepseek_metadiscourse_annotation  \\\n",
      "0  ```json\\n{\\n   \"role\": \"Metadiscourse\",\\n   \"c...   \n",
      "1  ```json\\n{\\n   \"role\": \"Metadiscourse\",\\n   \"c...   \n",
      "2  ```json\\n{\\n  \"role\": \"Metadiscourse\",\\n  \"con...   \n",
      "3  ```json\\n{\\n   \"role\": \"Propositional\",\\n   \"c...   \n",
      "4  ```json\\n{\\n   \"role\": \"Metadiscourse\",\\n   \"c...   \n",
      "\n",
      "                                      context_before  \\\n",
      "0  Moreover, Scacco (2011) analysed Obama’s radio...   \n",
      "1  Ultimately, the study demonstrates that the ge...   \n",
      "2  1.2 The Function of Language, Politics, and Di...   \n",
      "3  1.2 The Function of Language, Politics, and Di...   \n",
      "4  1.2 The Function of Language, Politics, and Di...   \n",
      "\n",
      "                                       context_after  \\\n",
      "0  Genre: American Presidential Radio Address Sub...   \n",
      "1  1.2 The Function of Language, Politics, and Di...   \n",
      "2  This study stresses the importance of language...   \n",
      "3  Further, Hasan (2001: 8) maintains that contex...   \n",
      "4  This study stresses the importance of language...   \n",
      "\n",
      "                            optimized_final_decision           role  \\\n",
      "0  {\"role\": \"Metadiscourse\", \"confidence\": 5, \"no...  Metadiscourse   \n",
      "1  {\"role\": \"Metadiscourse\", \"confidence\": 5, \"no...  Metadiscourse   \n",
      "2  {\"role\": \"Borderline\", \"confidence\": 4, \"note\"...     Borderline   \n",
      "3  {\"role\": \"Borderline\", \"confidence\": 4, \"note\"...     Borderline   \n",
      "4  {\"role\": \"Borderline\", \"confidence\": 4, \"note\"...     Borderline   \n",
      "\n",
      "   confidence                                               note  \\\n",
      "0         5.0  Pure textual navigation function with no propo...   \n",
      "1         5.0  Unanimous agreement on role and function, with...   \n",
      "2         4.0  The expression 'First... then...' serves both ...   \n",
      "3         4.0  The expression 'Halliday...explains...' serves...   \n",
      "4         4.0  The expression 'maintains that' serves both as...   \n",
      "\n",
      "                                       justification  \\\n",
      "0  The expression 'Below' functions exclusively a...   \n",
      "1  The expression 'Finally' serves as a sequentia...   \n",
      "2  The expression 'First... then...' is used to s...   \n",
      "3  The expression 'Halliday...explains...' introd...   \n",
      "4  The expression 'maintains that' is used to pre...   \n",
      "\n",
      "                                  context_assessment  \n",
      "0  Context is fully sufficient for classification...  \n",
      "1  Context is sufficient for classification. The ...  \n",
      "2  The context is sufficient for classification. ...  \n",
      "3  The context is sufficient for classification. ...  \n",
      "4  The context is sufficient for classification. ...  \n"
     ]
    }
   ],
   "source": [
    "# Load human annotations\n",
    "# Adjust the file path and format as needed\n",
    "try:\n",
    "    human_df = pd.read_csv('../result/human/Human-annotation-test_restricted_FIXED.csv')  # Adjust path as needed\n",
    "except FileNotFoundError:\n",
    "    try:\n",
    "        human_df = pd.read_csv('../result/human/Human-annotation-test_restricted_FIXED.csv', sep='\\t')  # Try tab-separated\n",
    "    except FileNotFoundError:\n",
    "        print(\"Please check the path to human annotations file\")\n",
    "        # print(f\"Here is the {human_df}\")\n",
    "        human_df = None\n",
    "\n",
    "\n",
    "# Load LLM annotations\n",
    "try:\n",
    "    llm_df = pd.read_csv('../result/flattened-annotation/flattened_optimized_annotations.csv')\n",
    "except FileNotFoundError:\n",
    "    print(\"Please check the path to LLM annotations file\")\n",
    "    llm_df = None\n",
    "\n",
    "# Display basic info about the datasets\n",
    "if human_df is not None:\n",
    "    print(\"Human Annotations:\")\n",
    "    print(f\"Shape: {human_df.shape}\")\n",
    "    print(f\"Columns: {list(human_df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(human_df.head())\n",
    "\n",
    "if llm_df is not None:\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"LLM Annotations:\")\n",
    "    print(f\"Shape: {llm_df.shape}\")\n",
    "    print(f\"Columns: {list(llm_df.columns)}\")\n",
    "    print(\"\\nFirst few rows:\")\n",
    "    print(llm_df.head())"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Human Role Distribution:\n",
      "role\n",
      "Metadiscourse    14\n",
      "Borderline       10\n",
      "Propositional     1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique roles in human data: ['Metadiscourse' 'Borderline' 'Propositional']\n",
      "\n",
      "========================================\n",
      "LLM Role Distribution:\n",
      "role\n",
      "Metadiscourse    13\n",
      "Borderline       10\n",
      "Propositional     2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Unique roles in LLM data: ['Metadiscourse' 'Borderline' 'Propositional']\n"
     ]
    }
   ],
   "source": [
    "# Examine the 'role' column in both datasets\n",
    "if human_df is not None and 'role' in human_df.columns:\n",
    "    print(\"Human Role Distribution:\")\n",
    "    print(human_df['role'].value_counts())\n",
    "    print(f\"\\nUnique roles in human data: {human_df['role'].unique()}\")\n",
    "else:\n",
    "    print(\"role column not found in human data or data not loaded\")\n",
    "    print(\"Available columns:\", list(human_df.columns) if human_df is not None else \"None\")\n",
    "\n",
    "if llm_df is not None and 'role' in llm_df.columns:\n",
    "    print(\"\\n\" + \"=\"*40)\n",
    "    print(\"LLM Role Distribution:\")\n",
    "    print(llm_df['role'].value_counts())\n",
    "    print(f\"\\nUnique roles in LLM data: {llm_df['role'].unique()}\")\n",
    "else:\n",
    "    print(\"\\nRole column not found in LLM data or data not loaded\")\n",
    "    print(\"Available columns:\", list(llm_df.columns) if llm_df is not None else \"None\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "# import pandas as pd\n",
    "#\n",
    "# # Read the Excel file\n",
    "# xlsx_file = '../result/human/Human-annotation.xlsx'  # Update with your actual path\n",
    "# df = pd.read_excel(xlsx_file)\n",
    "#\n",
    "# # Save as CSV\n",
    "# df.to_csv('../result/human/Human-annotation-test_restricted_FIXED.csv', index=False)\n",
    "#\n",
    "# print(f\"Converted! Shape: {df.shape}\")\n",
    "# print(f\"Columns: {list(df.columns)}\")\n",
    "#\n",
    "# # Check if role column exists\n",
    "# if 'role' in df.columns:\n",
    "#     print(\"✓ Role column found!\")\n",
    "#     print(df['role'].value_counts())\n",
    "# else:\n",
    "#     print(\"Available columns:\", list(df.columns))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Aligned dataset size: 25\n",
      "Sample alignment check:\n",
      "           Human            LLM\n",
      "0  Metadiscourse  Metadiscourse\n",
      "1  Metadiscourse  Metadiscourse\n",
      "2     Borderline     Borderline\n",
      "3  Metadiscourse     Borderline\n",
      "4  Metadiscourse     Borderline\n",
      "5  Metadiscourse     Borderline\n",
      "6  Metadiscourse  Metadiscourse\n",
      "7  Metadiscourse     Borderline\n",
      "8     Borderline  Metadiscourse\n",
      "9     Borderline  Metadiscourse\n"
     ]
    }
   ],
   "source": [
    "# Align datasets - this assumes both datasets have some common identifier\n",
    "\n",
    "def align_annotations(human_df, llm_df, id_column=None):\n",
    "    \"\"\"\n",
    "    Align human and LLM annotations based on a common identifier.\n",
    "    If no id_column is specified, assumes same order and length.\n",
    "    \"\"\"\n",
    "    if human_df is None or llm_df is None:\n",
    "        return None, None\n",
    "\n",
    "    if id_column and id_column in human_df.columns and id_column in llm_df.columns:\n",
    "        # Merge on common identifier\n",
    "        merged = pd.merge(human_df, llm_df, on=id_column, suffixes=('_human', '_llm'))\n",
    "        return merged['role_human'], merged['role_llm']\n",
    "    else:\n",
    "        # Assume same order and length\n",
    "        min_len = min(len(human_df), len(llm_df))\n",
    "        return human_df['role'][:min_len], llm_df['role'][:min_len]\n",
    "\n",
    "# Aligning the data based on sentence (adjust id_column as needed)\n",
    "\n",
    "human_roles, llm_roles = align_annotations(human_df, llm_df, id_column=None)\n",
    "\n",
    "if human_roles is not None and llm_roles is not None:\n",
    "    print(f\"Aligned dataset size: {len(human_roles)}\")\n",
    "    print(f\"Sample alignment check:\")\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Human': human_roles.head(10),\n",
    "        'LLM': llm_roles.head(10)\n",
    "    })\n",
    "    print(comparison_df)\n",
    "else:\n",
    "    print(\"Could not align the datasets. Please check data loading and column names.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence alignment analysis:\n",
      "  - Total sentences in human data: 24\n",
      "  - Total sentences in LLM data: 24\n",
      "  - Common sentences (exact matches): 24\n",
      "  - Only in human data: 0\n",
      "  - Only in LLM data: 0\n",
      "\n",
      "Successfully aligned 27 sentences!\n",
      "\n",
      "First 10 aligned sentences:\n",
      "1. Human: Metadiscourse | LLM: Metadiscourse\n",
      "   Sentence: This thesis focuses on ‘text as an instrument’ and aims to reveal the generic st...\n",
      "\n",
      "2. Human: Metadiscourse | LLM: Metadiscourse\n",
      "   Sentence: This study takes the standpoint that genre is ‘a staged, goal oriented, purposef...\n",
      "\n",
      "3. Human: Borderline | LLM: Borderline\n",
      "   Sentence: Scacci (2011) states, ‘genres exist in relation to certain situational constrain...\n",
      "\n",
      "4. Human: Metadiscourse | LLM: Metadiscourse\n",
      "   Sentence: The present study aims to fill this gap within studies of this particular genre....\n",
      "\n",
      "5. Human: Metadiscourse | LLM: Metadiscourse\n",
      "   Sentence: Below, Figure 1.1 visualises this:...\n",
      "\n",
      "6. Human: Metadiscourse | LLM: Metadiscourse\n",
      "   Sentence: This introductory chapter will first explain the functions of language, politics...\n",
      "\n",
      "7. Human: Metadiscourse | LLM: Metadiscourse\n",
      "   Sentence: Next, the main theoretical concepts of the study will be mapped out, from genre,...\n",
      "\n",
      "8. Human: Metadiscourse | LLM: Metadiscourse\n",
      "   Sentence: Finally, the structure of the overall thesis will be explained....\n",
      "\n",
      "9. Human: Borderline | LLM: Borderline\n",
      "   Sentence: Further, Hasan (2001: 8) maintains that context (of situation) is important in t...\n",
      "\n",
      "10. Human: Borderline | LLM: Borderline\n",
      "   Sentence: Halliday (2009:56) explains that language provides the means by which we act as ...\n",
      "\n",
      "\n",
      "Final aligned dataset size: 27\n",
      "\n",
      "Alignment summary:\n",
      "  - Total aligned annotations: 27\n",
      "  - Perfect matches: 26\n",
      "  - Disagreements: 1\n",
      "  - Agreement rate (accuracy): 0.9630 (96.30%)\n"
     ]
    }
   ],
   "source": [
    "def align_annotations(human_df, llm_df, id_column='sentence'):\n",
    "    \"\"\"\n",
    "    Align human and LLM annotations based on a common identifier.\n",
    "    If no id_column is specified, assumes same order and length.\n",
    "    \"\"\"\n",
    "    if human_df is None or llm_df is None:\n",
    "        return None, None, None\n",
    "\n",
    "    if id_column and id_column in human_df.columns and id_column in llm_df.columns:\n",
    "        # First, let's check how many sentences match exactly\n",
    "        human_sentences = set(human_df[id_column])\n",
    "        llm_sentences = set(llm_df[id_column])\n",
    "\n",
    "        common_sentences = human_sentences.intersection(llm_sentences)\n",
    "        only_human = human_sentences - llm_sentences\n",
    "        only_llm = llm_sentences - human_sentences\n",
    "\n",
    "        print(f\"Sentence alignment analysis:\")\n",
    "        print(f\"  - Total sentences in human data: {len(human_sentences)}\")\n",
    "        print(f\"  - Total sentences in LLM data: {len(llm_sentences)}\")\n",
    "        print(f\"  - Common sentences (exact matches): {len(common_sentences)}\")\n",
    "        print(f\"  - Only in human data: {len(only_human)}\")\n",
    "        print(f\"  - Only in LLM data: {len(only_llm)}\")\n",
    "\n",
    "        if len(common_sentences) == 0:\n",
    "            print(\"\\nNo exact sentence matches found. Let's check the first 10 sentences from each:\")\n",
    "            print(\"\\nFirst 10 Human sentences:\")\n",
    "            for i, sent in enumerate(human_df[id_column].head(10)):\n",
    "                print(f\"  {i+1}: {sent[:100]}...\")\n",
    "\n",
    "            print(\"\\nFirst 10 LLM sentences:\")\n",
    "            for i, sent in enumerate(llm_df[id_column].head(10)):\n",
    "                print(f\"  {i+1}: {sent[:100]}...\")\n",
    "\n",
    "            return None, None, None\n",
    "\n",
    "        # Merge on sentence column\n",
    "        merged = pd.merge(human_df, llm_df, on=id_column, suffixes=('_human', '_llm'))\n",
    "\n",
    "        print(f\"\\nSuccessfully aligned {len(merged)} sentences!\")\n",
    "\n",
    "        # Show first 10 aligned sentences\n",
    "        print(\"\\nFirst 10 aligned sentences:\")\n",
    "        for i in range(min(10, len(merged))):\n",
    "            print(f\"{i+1}. Human: {merged['role_human'].iloc[i]} | LLM: {merged['role_llm'].iloc[i]}\")\n",
    "            print(f\"   Sentence: {merged[id_column].iloc[i][:80]}...\")\n",
    "            print()\n",
    "\n",
    "        return merged['role_human'], merged['role_llm'], merged\n",
    "    else:\n",
    "        print(f\"Column '{id_column}' not found in one or both datasets\")\n",
    "        print(\"Human columns:\", list(human_df.columns) if human_df is not None else \"None\")\n",
    "        print(\"LLM columns:\", list(llm_df.columns) if llm_df is not None else \"None\")\n",
    "\n",
    "        # Fallback to positional alignment\n",
    "        min_len = min(len(human_df), len(llm_df))\n",
    "        return human_df['role'][:min_len], llm_df['role'][:min_len], None\n",
    "\n",
    "# Try to align the data using sentence column\n",
    "human_roles, llm_roles, merged_df = align_annotations(human_df, llm_df, id_column='sentence')\n",
    "\n",
    "if human_roles is not None and llm_roles is not None:\n",
    "    print(f\"\\nFinal aligned dataset size: {len(human_roles)}\")\n",
    "\n",
    "    # Create comparison DataFrame for analysis\n",
    "    comparison_df = pd.DataFrame({\n",
    "        'Human_Role': human_roles,\n",
    "        'LLM_Role': llm_roles,\n",
    "        'Agreement': human_roles == llm_roles\n",
    "    })\n",
    "\n",
    "    print(f\"\\nAlignment summary:\")\n",
    "    print(f\"  - Total aligned annotations: {len(comparison_df)}\")\n",
    "    print(f\"  - Perfect matches: {comparison_df['Agreement'].sum()}\")\n",
    "    print(f\"  - Disagreements: {(~comparison_df['Agreement']).sum()}\")\n",
    "    print(f\"  - Agreement rate (accuracy): {comparison_df['Agreement'].mean():.4f} ({comparison_df['Agreement'].mean()*100:.2f}%)\")\n",
    "\n",
    "else:\n",
    "    print(\"Could not align the datasets. Please check data loading and column names.\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Valid annotations for comparison: 27\n",
      "\n",
      "1. Raw Agreement (Accuracy): 0.9630 (96.30%)\n",
      "2. Cohen's Kappa: 0.9302\n",
      "   Interpretation: Almost Perfect\n",
      "\n",
      "3. Detailed Classification Report:\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "   Borderline       1.00      1.00      1.00        10\n",
      "Metadiscourse       1.00      0.94      0.97        16\n",
      "Propositional       0.50      1.00      0.67         1\n",
      "\n",
      "     accuracy                           0.96        27\n",
      "    macro avg       0.83      0.98      0.88        27\n",
      " weighted avg       0.98      0.96      0.97        27\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Calculate agreement metrics\n",
    "if human_roles is not None and llm_roles is not None:\n",
    "    # Remove any NaN values\n",
    "    valid_indices = pd.notna(human_roles) & pd.notna(llm_roles)\n",
    "    human_clean = human_roles[valid_indices]\n",
    "    llm_clean = llm_roles[valid_indices]\n",
    "\n",
    "    print(f\"Valid annotations for comparison: {len(human_clean)}\")\n",
    "\n",
    "    # 1. Raw Agreement (Accuracy)\n",
    "    accuracy = accuracy_score(human_clean, llm_clean)\n",
    "    print(f\"\\n1. Raw Agreement (Accuracy): {accuracy:.4f} ({accuracy*100:.2f}%)\")\n",
    "\n",
    "    # 2. Cohen's Kappa\n",
    "    kappa = cohen_kappa_score(human_clean, llm_clean)\n",
    "    print(f\"2. Cohen's Kappa: {kappa:.4f}\")\n",
    "\n",
    "    # Kappa interpretation\n",
    "    if kappa < 0:\n",
    "        kappa_interp = \"Poor (worse than chance)\"\n",
    "    elif kappa < 0.2:\n",
    "        kappa_interp = \"Slight\"\n",
    "    elif kappa < 0.4:\n",
    "        kappa_interp = \"Fair\"\n",
    "    elif kappa < 0.6:\n",
    "        kappa_interp = \"Moderate\"\n",
    "    elif kappa < 0.8:\n",
    "        kappa_interp = \"Substantial\"\n",
    "    else:\n",
    "        kappa_interp = \"Almost Perfect\"\n",
    "\n",
    "    print(f\"   Interpretation: {kappa_interp}\")\n",
    "\n",
    "    # 3. Per-class metrics\n",
    "    print(\"\\n3. Detailed Classification Report:\")\n",
    "    print(classification_report(human_clean, llm_clean, zero_division=0))\n",
    "\n",
    "else:\n",
    "    print(\"Cannot calculate metrics - data not available\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
